name: Kafka Async Polling E2E Tests

on:
  pull_request:
    paths:
      - 'bizon/connectors/sources/kafka/**'
      - 'tests/e2e/test_e2e_kafka_async_polling.py'
      - 'tests/connectors/sources/kafka/**'
      - '.github/workflows/kafka-e2e.yml'
  push:
    branches:
      - main
      - develop
    paths:
      - 'bizon/connectors/sources/kafka/**'
      - 'tests/e2e/test_e2e_kafka_async_polling.py'
      - 'tests/connectors/sources/kafka/**'
  workflow_dispatch: # Allow manual trigger

permissions:
  contents: read

jobs:
  kafka-e2e-test:
    name: Kafka Async Polling E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Start Kafka with Docker Compose
      run: |
        echo "Starting Kafka with Docker Compose..."
        cat > docker-compose.yml << 'EOF'
        version: '3.8'
        services:
          kafka:
            image: confluentinc/confluent-local:7.5.0
            hostname: kafka
            container_name: kafka
            ports:
              - "9092:9092"
            environment:
              KAFKA_NODE_ID: 1
              KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
              KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
              KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
              KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
              KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
              KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
              KAFKA_JMX_PORT: 9101
              KAFKA_JMX_HOSTNAME: localhost
              KAFKA_PROCESS_ROLES: 'broker,controller'
              KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
              KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
              KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
              KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
              KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
              CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
        EOF
        docker compose up -d
        echo "Waiting for container to start..."
        sleep 5
        docker compose ps

    - name: Wait for Kafka to be ready
      run: |
        echo "Waiting for Kafka to be ready..."
        # Check container logs
        echo "Container logs:"
        docker compose logs kafka | tail -20
        # Wait for Kafka port to be available
        echo "Waiting for Kafka port 9092..."
        timeout 120s bash -c 'until nc -z localhost 9092; do echo "Waiting..."; sleep 3; done'
        # Additional wait for Kafka to fully initialize
        echo "Waiting for Kafka to fully initialize..."
        sleep 15
        # Check logs again
        echo "Final container logs:"
        docker compose logs kafka | tail -10
        echo "Kafka should be ready!"

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true

    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}

    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --with test --all-extras

    - name: Verify Kafka connection
      run: |
        echo "Verifying Kafka connection with retries..."
        poetry run python -c "
        from confluent_kafka import Producer
        import time
        import sys
        
        for attempt in range(1, 6):
            try:
                print(f'Attempt {attempt}/5: Testing Kafka connection...')
                producer = Producer({
                    'bootstrap.servers': 'localhost:9092',
                    'client.id': 'verification-client',
                    'api.version.request': True,
                    'api.version.request.timeout.ms': 10000
                })
                producer.produce('test-topic', 'test-message')
                producer.flush(timeout=10)
                print('âœ… Kafka connection verified!')
                break
            except Exception as e:
                print(f'âŒ Attempt {attempt} failed: {e}')
                if attempt == 5:
                    print('Failed to connect after 5 attempts')
                    sys.exit(1)
                time.sleep(5)
        "

    - name: Create test topics and produce data
      run: |
        poetry run python -c "
        import json
        import time
        from confluent_kafka import Producer
        from confluent_kafka.admin import AdminClient, NewTopic
        
        print('ğŸ¯ Setting up Kafka test environment...')
        # Create topics
        admin_client = AdminClient({'bootstrap.servers': 'localhost:9092'})
        topics = [
            NewTopic('async-test-high-volume', num_partitions=3, replication_factor=1),
            NewTopic('async-test-low-volume', num_partitions=1, replication_factor=1)
        ]
        
        futures = admin_client.create_topics(topics)
        for topic_name, future in futures.items():
            try:
                future.result(timeout=10)
                print(f'âœ… Created topic: {topic_name}')
            except Exception as e:
                if 'already exists' in str(e).lower():
                    print(f'â„¹ï¸  Topic {topic_name} already exists')
                else:
                    print(f'âŒ Failed to create topic {topic_name}: {e}')
                    raise
        # Produce test data
        producer = Producer({
            'bootstrap.servers': 'localhost:9092',
            'client.id': 'ci-test-producer'
        })
        
        print('ğŸš€ Producing test data to simulate starvation scenario...')
        # High-volume topic - simulate heavy load (200 messages)
        for i in range(200):
            message = {
                'id': f'high-vol-{i}',
                'type': 'high_volume',
                'data': f'High volume message {i}',
                'timestamp': int(time.time() * 1000),
                'batch': i // 20
            }
            
            producer.produce(
                'async-test-high-volume',
                key=json.dumps(f'high-vol-{i}'),
                value=json.dumps(message),
                partition=i % 3
            )
            
            if i % 50 == 0:
                producer.flush()
        # Low-volume topic - simulate critical but infrequent messages (10 messages)
        for i in range(10):
            message = {
                'id': f'low-vol-{i}',
                'type': 'low_volume',
                'data': f'Critical message {i}',
                'priority': 'HIGH',
                'timestamp': int(time.time() * 1000),
                'sequence': i
            }
            
            producer.produce(
                'async-test-low-volume',
                key=json.dumps(f'low-vol-{i}'),
                value=json.dumps(message),
                partition=0
            )
        
        producer.flush()
        print('ğŸ“¦ Test data produced successfully!')
        print('   - High-volume topic: 200 messages across 3 partitions')
        print('   - Low-volume topic: 10 critical messages in 1 partition')
        "

    - name: Run Kafka async polling unit tests
      run: |
        echo "ğŸ§ª Running async polling unit tests..."
        poetry run pytest tests/connectors/sources/kafka/test_async_polling.py -v --tb=short

    - name: Run Kafka async polling integration test
      run: |
        echo "ğŸ”¬ Running async polling integration test..."
        poetry run python -c "
        import json
        import tempfile
        import os
        from pathlib import Path
        
        # Create test output directory
        test_output_dir = tempfile.mkdtemp(prefix='kafka_async_test_')
        print(f'ğŸ“ Test output directory: {test_output_dir}')
        
        # Test configuration
        config_yaml = '''
        name: ci_kafka_async_polling_test
        source:
          name: kafka
          stream: topic
          sync_mode: stream
          topics:
            - name: async-test-high-volume
              destination_id: high_volume
            - name: async-test-low-volume
              destination_id: low_volume
          bootstrap_servers: localhost:9092
          group_id: ci-async-test-group
          batch_size: 30
          consumer_timeout: 2
          enable_async_polling: true
          poll_interval_ms: 50
          max_poll_records: 100
          partition_pause_threshold: 500
          message_encoding: utf-8
          max_iterations: 8
          consumer_config:
            security.protocol: PLAINTEXT
            sasl.mechanism: PLAIN
            auto.offset.reset: earliest
          authentication:
            type: basic
            params:
              username: test
              password: test
            schema_registry_type: apicurio
            schema_registry_url: "http://localhost:8081"
            schema_registry_username: "test"
            schema_registry_password: "test"
        destination:
          name: file
          config:
            format: json
            destination_id: kafka_test
            record_schemas:
              - destination_id: high_volume
                record_schema:
                  - name: id
                    type: string
                    nullable: false
                  - name: type
                    type: string
                    nullable: false
                  - name: data
                    type: string
                    nullable: false
                  - name: timestamp
                    type: integer
                    nullable: false
              - destination_id: low_volume
                record_schema:
                  - name: id
                    type: string
                    nullable: false
                  - name: type
                    type: string
                    nullable: false
                  - name: data
                    type: string
                    nullable: false
                  - name: priority
                    type: string
                    nullable: false
                  - name: timestamp
                    type: integer
                    nullable: false
        engine:
          runner:
            type: stream
          backend:
            type: sqlite
            config:
              database: bizon_test
              schema: public
        '''
        
        # Write config to file
        config_file = os.path.join(test_output_dir, 'config.yml')
        with open(config_file, 'w') as f:
            f.write(config_yaml)
        
        print(f'ğŸ“„ Config written to: {config_file}')
        
        # Clean up any existing output files from previous runs
        for cleanup_file in ['high_volume.json', 'low_volume.json', 'kafka_test.json']:
            if os.path.exists(cleanup_file):
                os.remove(cleanup_file)
        
        # Set environment for production mode (enables commits)
        os.environ['ENVIRONMENT'] = 'production'
        
        # Run bizon
        print('ğŸš€ Running bizon with async polling...')
        
        import yaml
        from bizon.engine.engine import RunnerFactory
        
        with open(config_file, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        runner = RunnerFactory.create_from_config_dict(config_dict)
        result = runner.run()
        
        print(f'âœ… Bizon completed with status: {result}')
        
        # Analyze results
        print('ğŸ“Š Analyzing consumption results...')
        
        # List all files in both output directory and current working directory
        print(f'ğŸ“ Files in {test_output_dir}:')
        for file in os.listdir(test_output_dir):
            file_path = os.path.join(test_output_dir, file)
            if os.path.isfile(file_path):
                size = os.path.getsize(file_path)
                print(f'   - {file} ({size} bytes)')
        
        print(f'ğŸ“ Files in current working directory:')
        for file in os.listdir('.'):
            if os.path.isfile(file):
                size = os.path.getsize(file)
                print(f'   - {file} ({size} bytes)')
        
        # Look for files in current directory (where file destination writes)
        # Files are created based on destination_id from record schemas, not main destination_id
        high_vol_file = 'high_volume.json'
        low_vol_file = 'low_volume.json'
        
        high_vol_count = 0
        low_vol_count = 0
        
        if os.path.exists(high_vol_file):
            with open(high_vol_file, 'r') as f:
                high_vol_count = sum(1 for line in f if line.strip())
        else:
            print(f'âŒ {high_vol_file} does not exist')
        
        if os.path.exists(low_vol_file):
            with open(low_vol_file, 'r') as f:
                low_vol_count = sum(1 for line in f if line.strip())
        else:
            print(f'âŒ {low_vol_file} does not exist')
        
        total_consumed = high_vol_count + low_vol_count
        
        print(f'ğŸ“ˆ Consumption Results:')
        print(f'   High-volume messages: {high_vol_count}')
        print(f'   Low-volume messages:  {low_vol_count}')
        print(f'   Total consumed:       {total_consumed}')
        
        if total_consumed > 0:
            high_vol_pct = (high_vol_count / total_consumed) * 100
            low_vol_pct = (low_vol_count / total_consumed) * 100
            
            print(f'ğŸ“Š Fairness Analysis:')
            print(f'   High-volume: {high_vol_pct:.1f}%')
            print(f'   Low-volume:  {low_vol_pct:.1f}%')
        
        # Assertions
        assert high_vol_count > 0, f'High-volume topic should have consumed messages, got {high_vol_count}'
        assert low_vol_count > 0, f'Low-volume topic should NOT be starved, got {low_vol_count}'
        
        # Key test: Fair representation despite volume difference
        if total_consumed > 0:
            fairness_ratio = low_vol_count / total_consumed
            assert fairness_ratio > 0.01, f'Low-volume topic got {fairness_ratio:.2%} - indicates starvation!'
        
        print(f'âœ… SUCCESS: Async polling prevented topic starvation!')
        print(f'   Low-volume topic maintained fair representation despite high-volume load')
        
        # Cleanup
        import shutil
        shutil.rmtree(test_output_dir)
        print(f'ğŸ§¹ Cleaned up test directory')
        "
      env:
        PYTHONPATH: /home/runner/work/bizon-core/bizon-core

    - name: Test starvation comparison (sync vs async)
      run: |
        echo "âš–ï¸  Testing sync vs async polling comparison..."
        poetry run python -c "
        import json
        import tempfile
        import os
        import shutil
        from pathlib import Path
        
        def run_bizon_test(polling_mode, test_name):
            '''Run bizon with specified polling mode'''
            
            test_output_dir = tempfile.mkdtemp(prefix=f'kafka_{polling_mode}_test_')
            print(f'ğŸ“ {test_name} output directory: {test_output_dir}')
            
            enable_async = (polling_mode == 'async')
            
            config_yaml = f'''
            name: ci_kafka_{polling_mode}_test
            source:
              name: kafka
              stream: topic
              sync_mode: stream
              topics:
                - name: async-test-high-volume
                  destination_id: high_volume
                - name: async-test-low-volume
                  destination_id: low_volume
              bootstrap_servers: localhost:9092
              group_id: ci-{polling_mode}-test-group
              batch_size: 25
              consumer_timeout: 1
              enable_async_polling: {str(enable_async).lower()}
              poll_interval_ms: 50
              max_poll_records: 50
              message_encoding: utf-8
              max_iterations: 5
              consumer_config:
                security.protocol: PLAINTEXT
                sasl.mechanism: PLAIN
                auto.offset.reset: earliest
              authentication:
                type: basic
                params:
                  username: test
                  password: test
                schema_registry_type: apicurio
                schema_registry_url: "http://localhost:8081"
                schema_registry_username: "test"
                schema_registry_password: "test"
            destination:
              name: file
              config:
                format: json
                destination_id: kafka_comparison_test
                record_schemas:
                  - destination_id: high_volume
                    record_schema:
                      - name: id
                        type: string
                        nullable: false
                      - name: type  
                        type: string
                        nullable: false
                  - destination_id: low_volume
                    record_schema:
                      - name: id
                        type: string
                        nullable: false
                      - name: type
                        type: string
                        nullable: false
            engine:
              runner:
                type: stream
              backend:
                type: sqlite
                config:
                  database: bizon_test
                  schema: public
            '''
            
            # Write and run config
            config_file = os.path.join(test_output_dir, 'config.yml')
            with open(config_file, 'w') as f:
                f.write(config_yaml)
            
            # Clean up any existing output files from previous runs
            for cleanup_file in ['high_volume.json', 'low_volume.json', 'kafka_comparison_test.json']:
                if os.path.exists(cleanup_file):
                    os.remove(cleanup_file)
            
            os.environ['ENVIRONMENT'] = 'production'
            
            import yaml
            from bizon.engine.engine import RunnerFactory
            
            with open(config_file, 'r') as f:
                config_dict = yaml.safe_load(f)
            
            runner = RunnerFactory.create_from_config_dict(config_dict)
            runner.run()
            
            # Count results  
            print(f'ğŸ“ Files in {test_output_dir}:')
            for file in os.listdir(test_output_dir):
                file_path = os.path.join(test_output_dir, file)
                if os.path.isfile(file_path):
                    size = os.path.getsize(file_path)
                    print(f'   - {file} ({size} bytes)')
            
            print(f'ğŸ“ Files in current working directory:')
            for file in os.listdir('.'):
                if os.path.isfile(file):
                    size = os.path.getsize(file)
                    print(f'   - {file} ({size} bytes)')
            
            high_vol_file = 'high_volume.json'
            low_vol_file = 'low_volume.json'
            
            high_vol_count = 0
            low_vol_count = 0
            
            if os.path.exists(high_vol_file):
                with open(high_vol_file, 'r') as f:
                    high_vol_count = sum(1 for line in f if line.strip())
            
            if os.path.exists(low_vol_file):
                with open(low_vol_file, 'r') as f:
                    low_vol_count = sum(1 for line in f if line.strip())
            
            # Cleanup
            shutil.rmtree(test_output_dir)
            
            return high_vol_count, low_vol_count
        
        # Run both tests
        print('ğŸ”„ Running sync polling test...')
        sync_high, sync_low = run_bizon_test('sync', 'Sync Polling')
        
        print('ğŸ”„ Running async polling test...')  
        async_high, async_low = run_bizon_test('async', 'Async Polling')
        
        # Compare results
        print(f'')
        print(f'ğŸ“Š Polling Mode Comparison Results:')
        print(f'   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”')
        print(f'   â”‚ Polling Mode    â”‚ High Volume â”‚ Low Volume  â”‚')
        print(f'   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤')
        print(f'   â”‚ Sync (old)      â”‚ {sync_high:11d} â”‚ {sync_low:11d} â”‚')
        print(f'   â”‚ Async (new)     â”‚ {async_high:11d} â”‚ {async_low:11d} â”‚')
        print(f'   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜')
        
        # Analysis
        sync_total = sync_high + sync_low
        async_total = async_high + async_low
        
        if sync_total > 0:
            sync_low_pct = (sync_low / sync_total) * 100
        else:
            sync_low_pct = 0
            
        if async_total > 0:
            async_low_pct = (async_low / async_total) * 100
        else:
            async_low_pct = 0
        
        print(f'')
        print(f'ğŸ“ˆ Starvation Analysis:')
        print(f'   Sync polling  - Low volume: {sync_low_pct:.1f}%')
        print(f'   Async polling - Low volume: {async_low_pct:.1f}%')
        
        # Key assertions
        assert async_low > 0, f'Async polling should consume low-volume messages, got {async_low}'
        assert async_low_pct >= sync_low_pct, f'Async should be >= sync fairness: {async_low_pct:.1f}% vs {sync_low_pct:.1f}%'
        
        if sync_low == 0 and async_low > 0:
            print(f'ğŸ‰ MAJOR SUCCESS: Async polling prevented complete starvation!')
        elif async_low > sync_low:
            improvement = ((async_low - sync_low) / max(sync_low, 1)) * 100
            print(f'ğŸ‰ SUCCESS: Async polling improved low-volume consumption by {improvement:.0f}%!')
        else:
            print(f'âœ… SUCCESS: Async polling maintained fair consumption')
        
        print(f'âœ… Starvation prevention test passed!')
        "
      env:
        PYTHONPATH: /home/runner/work/bizon-core/bizon-core

    - name: Verify Kafka consumer groups
      if: always()
      run: |
        echo "ğŸ‘¥ Checking Kafka consumer groups created during test..."
        poetry run python -c "
        from confluent_kafka.admin import AdminClient
        
        admin_client = AdminClient({'bootstrap.servers': 'localhost:9092'})
        
        try:
            groups = admin_client.list_consumer_groups(timeout=10)
            print(f'ğŸ“‹ Consumer groups found: {len(groups)}')
            for group in groups:
                print(f'   - {group}')
        except Exception as e:
            print(f'â„¹ï¸  Could not list consumer groups: {e}')
        "

    - name: Cleanup test topics
      if: always()
      run: |
        echo "ğŸ§¹ Cleaning up test topics..."
        poetry run python -c "
        from confluent_kafka.admin import AdminClient
        
        admin_client = AdminClient({'bootstrap.servers': 'localhost:9092'})
        
        topics_to_delete = ['async-test-high-volume', 'async-test-low-volume', 'test-topic']
        
        try:
            futures = admin_client.delete_topics(topics_to_delete, timeout=10)
            for topic, future in futures.items():
                try:
                    future.result()
                    print(f'ğŸ—‘ï¸  Deleted topic: {topic}')
                except Exception as e:
                    print(f'â„¹ï¸  Could not delete topic {topic}: {e}')
        except Exception as e:
            print(f'â„¹ï¸  Cleanup completed with some issues: {e}')
        "

    - name: Stop Kafka container
      if: always()
      run: |
        echo "ğŸ›‘ Stopping Kafka container..."
        docker compose down || true

    - name: Test Summary
      if: always()
      run: |
        echo "ğŸ¯ Kafka Async Polling E2E Test Summary"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âœ… Kafka KRaft cluster setup and connectivity"
        echo "âœ… Test data production (high/low volume)"
        echo "âœ… Async polling unit tests"  
        echo "âœ… Integration test with file destination"
        echo "âœ… Starvation prevention validation"
        echo "âœ… Sync vs async polling comparison"
        echo ""
        echo "ğŸš€ All tests passed! Async polling prevents topic starvation."